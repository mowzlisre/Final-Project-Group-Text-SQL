{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    return text.lower().strip()\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['Phrase'] = df['Phrase'].apply(preprocess_text)\n",
    "    phrases = df['Phrase'].tolist()\n",
    "    sql_queries = df['SQL'].tolist()\n",
    "    labels = df.iloc[:, 2:].values  # SQL syntax labels\n",
    "    label_columns = df.columns[2:]\n",
    "    return phrases, labels, sql_queries, label_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_seq2seq_data(phrases, labels, sql_queries, label_columns):\n",
    "    inputs = [\n",
    "        f\"SQL prediction: {phrase} [Labels: {', '.join([f'{label}:{val}' for label, val in zip(label_columns, label_row)])}]\"\n",
    "        for phrase, label_row in zip(phrases, labels)\n",
    "    ]\n",
    "    return inputs, sql_queries\n",
    "\n",
    "# Seq2Seq Dataset class\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, tokenizer, max_length=128):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        target_text = self.targets[idx]\n",
    "        input_encodings = self.tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "        target_encodings = self.tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": input_encodings[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": target_encodings[\"input_ids\"].squeeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLSeq2SeqModel:\n",
    "    def __init__(self, model_name=\"t5-small\"):\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "    def preprocess(self, phrase, labels, label_columns):\n",
    "        label_text = \", \".join([f\"{label}:{value}\" for label, value in zip(label_columns, labels)])\n",
    "        input_text = f\"Prompt: {phrase} Using Labels: {label_text}]\"\n",
    "        return input_text\n",
    "\n",
    "    def predict(self, phrase, labels, label_columns, max_length=128):\n",
    "        input_text = self.preprocess(phrase, labels, label_columns)\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        output_ids = self.model.generate(inputs[\"input_ids\"], max_length=max_length, num_beams=4, early_stopping=True)\n",
    "        return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs=10):\n",
    "    model.model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss = evaluate_model(model, val_loader)\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achaarya/Desktop/nlp-ollama/.venv/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.7079, Val Loss: 0.1236\n",
      "Epoch 2: Train Loss: 0.0772, Val Loss: 0.0612\n",
      "Epoch 3: Train Loss: 0.0483, Val Loss: 0.0484\n",
      "Epoch 4: Train Loss: 0.0369, Val Loss: 0.0432\n",
      "Epoch 5: Train Loss: 0.0298, Val Loss: 0.0405\n",
      "Epoch 6: Train Loss: 0.0250, Val Loss: 0.0387\n",
      "Epoch 7: Train Loss: 0.0211, Val Loss: 0.0378\n",
      "Epoch 8: Train Loss: 0.0182, Val Loss: 0.0383\n",
      "Epoch 9: Train Loss: 0.0167, Val Loss: 0.0378\n",
      "Epoch 10: Train Loss: 0.0145, Val Loss: 0.0379\n",
      "Predicted SQL: SELECT * FROM customers WHERE name='[customer_name]';\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    filepath = \"dataset.csv\" \n",
    "    phrases, labels, sql_queries, label_columns = load_dataset(filepath)\n",
    "\n",
    "    # Prepare data\n",
    "    inputs, targets = prepare_seq2seq_data(phrases, labels, sql_queries, label_columns)\n",
    "    train_inputs, val_inputs, train_targets, val_targets = train_test_split(inputs, targets, test_size=0.3, random_state=39)\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "    train_dataset = Seq2SeqDataset(train_inputs, train_targets, tokenizer)\n",
    "    val_dataset = Seq2SeqDataset(val_inputs, val_targets, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    model = SQLSeq2SeqModel(model_name=\"t5-small\")\n",
    "    optimizer = AdamW(model.model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_loader, val_loader, optimizer, num_epochs=10)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.model.state_dict(), \"./model/context_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
